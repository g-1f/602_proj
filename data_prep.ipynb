{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "census = pd.read_csv('census.csv', index_col='cfips')\n",
    "df_cleaned = pd.read_csv('df_cleaned.csv')\n",
    "df = pd.merge(train_df, df_cleaned, on=['cfips', 'row_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.rename(columns={'< 30k pop': 'l_30k', '> 225k pop': 'g_225k'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = df[['cfips', 'state', 'county']]\n",
    "state_dict = state_dict.set_index('cfips')\n",
    "state_dict = state_dict.drop_duplicates()\n",
    "state_dict = state_dict.to_dict()\n",
    "\n",
    "zero_mask = (df['microbusiness_density'] == 0) & (df['active'] == 0) \n",
    "df['ratio'] = df.loc[~ zero_mask].groupby('cfips', group_keys=False).apply(lambda x: x.active / x.microbusiness_density)\n",
    "df['ratio'] = df['ratio'].fillna(0)\n",
    "df.state = df['state'].astype('category').cat.codes\n",
    "df = df.drop(columns=['county'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first_day_of_month = pd.to_datetime(df.first_day_of_month)\n",
    "df['month'] = df.first_day_of_month.dt.month\n",
    "df['year'] = df.first_day_of_month.dt.year - 2019\n",
    "df['is_new_year'] = 0\n",
    "df.loc[df.month == 1, 'is_new_year'] = 1\n",
    "seasons = {1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring', 6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Fall', 10: 'Fall', 11: 'Fall', 12: 'Winter'}\n",
    "df['season'] = df['month'].apply(lambda x: seasons[x])\n",
    "df['md_national_avg'] = df.groupby(['year','month'])['microbusiness_density'].transform('mean')\n",
    "df['md_state_avg'] = df.groupby(['state','year','month'])['microbusiness_density'].transform('mean')\n",
    "df['a_national_avg'] = df.groupby(['year','month'])['active'].transform('mean')\n",
    "df['a_state_avg'] = df.groupby(['state','year','month'])['active'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['season'], prefix='season')], axis=1)\n",
    "df = df.drop(columns='season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "# Create a US holiday calendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "\n",
    "holidays = cal.holidays(start='2019-01-01', end='2023-06-01')\n",
    "df['holidays'] = pd.to_datetime(df['first_day_of_month'], format='%Y-%m-%d').dt.to_period('M').apply(lambda x: len(holidays[holidays.to_period('M') == x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['idx'] = df.groupby('cfips')['cfips'].cumcount()\n",
    "df = df.merge(census, on='cfips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_log_diff(df, groupby_col, column, mask, new_col_name):\n",
    "    df[new_col_name] = df.loc[~mask].groupby(groupby_col, group_keys=False)[column].apply(lambda x: np.log(x).diff())\n",
    "    df.loc[mask, [new_col_name]] = 0\n",
    "    df[new_col_name] = df.loc[~mask].groupby(groupby_col, group_keys=False)[new_col_name].bfill()\n",
    "    return df\n",
    "\n",
    "def create_lags(df, groupby_col, columns, lags):\n",
    "    for column in columns:\n",
    "        for lag in lags:\n",
    "            new_col_name = f'{column}_lag_{lag}'\n",
    "            df[new_col_name] = df.groupby(groupby_col, group_keys=False)[column].shift(lag)\n",
    "    return df\n",
    "\n",
    "def create_target(df, groupby_col, column, mask, new_col_name):\n",
    "    df[new_col_name] = df.loc[~mask].groupby(groupby_col, group_keys=False)[column].apply(lambda x: np.log(x.shift(-1)) - np.log(x))\n",
    "    df.loc[mask, [new_col_name]] = 0\n",
    "    return df\n",
    "\n",
    "def fill_target_with_train(df_test, train_df):\n",
    "    df_test = df_test.merge(train_df[['target1']], how='left', left_index=True, right_index=True)\n",
    "    df_test['target'] = df_test['target'].fillna(df_test['target1'])\n",
    "    df_test = df_test.drop(columns='target1')\n",
    "    return df_test\n",
    "\n",
    "# Preprocessing\n",
    "df = df.set_index('row_id')\n",
    "mask = ((df.microbusiness_density == 0) & (df.active == 0))\n",
    "\n",
    "# Calculate log differences\n",
    "df = calculate_log_diff(df, 'cfips', 'md_state_avg', mask, 'md_state_avg_log_diff')\n",
    "df = calculate_log_diff(df, 'cfips', 'a_state_avg', mask, 'a_state_avg_log_diff')\n",
    "\n",
    "# Create lags\n",
    "LAGS = [1, 2, 3, 4, 5]\n",
    "columns_to_lag = ['microbusiness_density', 'active']\n",
    "df = create_lags(df, 'cfips', columns_to_lag, LAGS)\n",
    "\n",
    "# Create target\n",
    "df = create_target(df, 'cfips', 'microbusiness_density', mask, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3085/3085 [00:01<00:00, 2015.50it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2102.78it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2189.40it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2217.10it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2135.40it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2120.26it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 1756.62it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2227.92it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2216.97it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2155.69it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2068.40it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2417.59it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2480.19it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2249.60it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2143.57it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2126.84it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2253.34it/s]\n",
      "100%|██████████| 3085/3085 [00:01<00:00, 2074.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_NN_cfips_by_feats(df, N, p, metric, feature):\n",
    "    \"\"\"\n",
    "    Get nearest neighbors based on a specific feature.\n",
    "    \"\"\"\n",
    "    df = df.reset_index()\n",
    "    df = df[['cfips', 'first_day_of_month', feature]]\n",
    "    df = df.pivot(index='cfips', columns='first_day_of_month', values=feature)\n",
    "    df.columns.name = None\n",
    "    df = df.fillna(0)\n",
    "    nn = NearestNeighbors(n_neighbors=N, p=p, metric=metric)\n",
    "    nn.fit(df)\n",
    "    neighbors = nn.kneighbors(df, return_distance=False)\n",
    "    df_nn = pd.DataFrame(neighbors, index=df.index)\n",
    "    df_nn = df_nn.apply(lambda col: df.index[col])\n",
    "    df_nn = df_nn.iloc[:, 1:]\n",
    "    return df_nn\n",
    "\n",
    "def get_NN_cfips_by_census(census, N, p, metric):\n",
    "    \"\"\"\n",
    "    Get nearest neighbors based on census data.\n",
    "    \"\"\"\n",
    "    nn_census = NearestNeighbors(n_neighbors=N, p=p, metric=metric)\n",
    "    census_na = census[census.isna().any(axis=1)]\n",
    "    na_mask = census_na.index\n",
    "    census.loc[na_mask] = census.loc[na_mask].fillna(method='ffill', axis=1)\n",
    "    df_mask = df.cfips.unique()\n",
    "    census = census.loc[df_mask]\n",
    "    nn_census.fit(census)\n",
    "    neighbors = nn_census.kneighbors(census, return_distance=False)\n",
    "    df_nn = pd.DataFrame(neighbors, index=census.index)\n",
    "    df_nn = df_nn.iloc[:, 1:]\n",
    "    df_nn = df_nn.loc[df_mask]\n",
    "    df_nn = df_nn.rename(columns={1: 'NN_1', 2: 'NN_2', 3: 'NN_3'})\n",
    "    df_nn[['NN_1', 'NN_2', 'NN_3']] = df_nn[['NN_1', 'NN_2', 'NN_3']].applymap(lambda x: census.index[x])\n",
    "    return df_nn\n",
    "\n",
    "def generate_feature(df_nn, df, feature, colname):\n",
    "    \"\"\"\n",
    "    Generate feature for nearest neighbors.\n",
    "    \"\"\"\n",
    "    cfips = df.cfips.unique()\n",
    "    df_grouped = df.groupby('cfips')\n",
    "    dfs = []\n",
    "    for cfips_val in tqdm(cfips):\n",
    "        nn_list = df_nn.loc[cfips_val].values.tolist()\n",
    "        feats = pd.DataFrame(np.asarray([df_grouped.get_group(i)[feature].values for i in nn_list]).T, columns=[f'{colname}_NN_{i}' for i in range(len(nn_list))])\n",
    "        dfs.append(feats)\n",
    "    result = pd.concat(dfs, axis=0)\n",
    "    result = result.reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "def combine_nn_feats(df, df_nn):\n",
    "    \"\"\"\n",
    "    Combine features with the original dataframe.\n",
    "    \"\"\"\n",
    "    df = df.reset_index()\n",
    "    df = pd.concat([df, df_nn], axis=1)\n",
    "    df = df.drop(columns='first_day_of_month')\n",
    "    df = df.set_index('row_id')\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_lagged_features(df_nn, df, feature, prefix):\n",
    "    feature_dfs = []\n",
    "    for lag in range(1, 6):  # Generate lags up to 5\n",
    "        colname = f\"{prefix}_lag_{lag}\"\n",
    "        feature_df = generate_feature(df_nn, df, f\"{feature}_lag_{lag}\", colname)\n",
    "        feature_dfs.append(feature_df)\n",
    "    return feature_dfs\n",
    "\n",
    "KNN = True\n",
    "\n",
    "if KNN:\n",
    "    # generate neighbor\n",
    "    df_nn_c = get_NN_cfips_by_census(census, N=4, p=1, metric='manhattan')\n",
    "    df_nn_md = get_NN_cfips_by_feats(df, N=4, p=1, metric='manhattan', feature='microbusiness_density')\n",
    "    df_nn_md_log_diff = get_NN_cfips_by_feats(df, N=4, p=1, metric='manhattan', feature='target')\n",
    "\n",
    "    # generate features\n",
    "    md_state_avg_diff_features = [\n",
    "        generate_feature(df_nn_c, df, 'md_state_avg_log_diff', 'md_state_avg_c'),\n",
    "        generate_feature(df_nn_md, df, 'md_state_avg_log_diff', 'md_state_avg_1'),\n",
    "        generate_feature(df_nn_md_log_diff, df, 'md_state_avg_log_diff', 'md_state_avg_ld'),\n",
    "    ]\n",
    "\n",
    "    md_lag_features_c = generate_lagged_features(df_nn_c, df, 'microbusiness_density', 'md_c')\n",
    "    md_lag_features_md = generate_lagged_features(df_nn_md, df, 'microbusiness_density', 'md_1')\n",
    "    md_lag_features_md_log_diff = generate_lagged_features(df_nn_md_log_diff, df, 'microbusiness_density', 'md_ld')\n",
    "\n",
    "    # Combine generated features\n",
    "    NN_feats_list = md_state_avg_diff_features + md_lag_features_c + md_lag_features_md + md_lag_features_md_log_diff\n",
    "    NN_feats = pd.concat(NN_feats_list, axis=1)\n",
    "    \n",
    "    # Merge the features with the training dataset\n",
    "    df = combine_nn_feats(df, NN_feats)\n",
    "\n",
    "else:\n",
    "    df = df.drop(columns='first_day_of_month')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "mask = ((train_df.microbusiness_density == 0) & (train_df.active == 0))\n",
    "train_df = create_target(train_df, 'cfips', 'microbusiness_density', mask, 'target1')\n",
    "train_df = train_df.set_index('row_id')\n",
    "df_train = df[~df.target.isna()]\n",
    "df_test = df[df.target.isna()]\n",
    "\n",
    "\n",
    "df_test = fill_target_with_train(df_test, train_df)\n",
    "\n",
    "df_train = df_train.fillna(0)\n",
    "target_train = df_train.target\n",
    "df_train.drop(columns='target', inplace=True)\n",
    "\n",
    "df_test = df_test.fillna(0)\n",
    "target_test = df_test.target\n",
    "df_test.drop(columns='target', inplace=True)\n",
    "\n",
    "df_train.to_csv('df_train.csv')\n",
    "df_test.to_csv('df_test.csv')\n",
    "target_train.to_csv('target_train.csv')\n",
    "target_test.to_csv('target_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
