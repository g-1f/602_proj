{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data import DGLDataset\n",
    "from geopy import distance\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighb = pd.read_csv('gnn_neighbor.csv')\n",
    "neighb['distances'] = neighb.apply(lambda x: ast.literal_eval(x.distances), axis=1)\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_train_NN = df_train[df_train.columns[df_train.columns.str.contains('NN')]]\n",
    "df_train = df_train.loc[:, ~df_train.columns.str.contains('NN')]\n",
    "target_train = pd.read_csv('target_train.csv')\n",
    "df_train = pd.merge(df_train, target_train, on='row_id')\n",
    "\n",
    "df_test = pd.read_csv('df_test.csv')\n",
    "df_test_NN = df_test[df_test.columns[df_test.columns.str.contains('NN')]]\n",
    "df_test = df_test.loc[:, ~df_test.columns.str.contains('NN')]\n",
    "target_test = pd.read_csv('target_test.csv')\n",
    "df_test = pd.merge(df_test, target_test, on='row_id')\n",
    "features = df_train.drop(['row_id', 'cfips', 'microbusiness_density', 'active'], axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.describe()\n",
    "df_train['cfips'] = df_train['cfips'].apply(lambda x: neighb.loc[neighb['original_cfips'] == x, 'cfips'].values[0])\n",
    "df_test['cfips'] = df_test['cfips'].apply(lambda x: neighb.loc[neighb['original_cfips'] == x, 'cfips'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(DGLDataset):\n",
    "    def __init__(self, edges, properties, features, train=True):\n",
    "        self.edges = edges\n",
    "        self.properties = properties\n",
    "        self.features = features\n",
    "        self.train = train\n",
    "        super().__init__(name=\"microbussiness\")\n",
    "\n",
    "    def process(self):\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for index, group in self.properties.groupby('idx'):\n",
    "            filtered_edges = self.edges[self.edges['cfips'].isin(group['cfips'])]\n",
    "\n",
    "            if filtered_edges.empty:\n",
    "                continue\n",
    "\n",
    "            src = []\n",
    "            dst = []\n",
    "            w = []\n",
    "\n",
    "            for _, row in filtered_edges.iterrows():\n",
    "                neighbours = ast.literal_eval(row['neighbors'])\n",
    "                distances = row['distances']\n",
    "\n",
    "                for neighbor, distance in zip(neighbours, distances):\n",
    "                    if neighbor in group['cfips'].values:\n",
    "                        src.append(row['cfips'])\n",
    "                        dst.append(neighbor)\n",
    "                        w.append(distance)\n",
    "\n",
    "            if len(w) == 0:\n",
    "                continue\n",
    "\n",
    "            w = np.array(w)\n",
    "            min_w = np.min(w)\n",
    "            max_w = np.max(w)\n",
    "            w = (w - min_w) / (max_w - min_w)\n",
    "\n",
    "            node_id_mapping = {node_id: i for i, node_id in enumerate(group['cfips'])}\n",
    "            src = [node_id_mapping[x] for x in src]\n",
    "            dst = [node_id_mapping[x] for x in dst]\n",
    "\n",
    "            g = dgl.graph((src, dst), num_nodes=len(group))\n",
    "            g.ndata['x'] = torch.tensor(group[self.features].values, dtype=torch.float) # node feature\n",
    "            g.edata['w'] = torch.tensor(w, dtype=torch.float)  # scalar integer feature\n",
    "            g = dgl.add_self_loop(g)\n",
    "            self.graphs.append(g)\n",
    "            if self.train:\n",
    "                self.labels.append(torch.tensor(group[\"target\"].values, dtype=torch.float))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.train:\n",
    "            return self.graphs[i].to(device), self.labels[i].to(device)\n",
    "\n",
    "        return self.graphs[i].to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "train_graph_dataset = GraphDataset(edges=neighb, properties=df_train, features=features)\n",
    "val_graph_dataset = GraphDataset(edges=neighb, properties=df_test, features=features,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GGN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GGN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.convh = nn.ModuleList([GraphConv(h_feats, h_feats) for _ in range(20)])\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        for c in self.convh:\n",
    "            h = c(g, h)\n",
    "            h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.tanh(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "ename": "DGLError",
     "evalue": "Invalid key \"0\". Must be one of the edge types.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/gyf/JHU/Spring 22/Data Mining/Project/602_proj/GNN.ipynb Cell 7\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m criterion \u001b[39m=\u001b[39m smape_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m train(model, train_loader, val_loader, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, optimizer\u001b[39m=\u001b[39;49moptimizer, criterion\u001b[39m=\u001b[39;49mcriterion, patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/gyf/JHU/Spring 22/Data Mining/Project/602_proj/GNN.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, optimizer, criterion, patience)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m val_loss \u001b[39m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{:04d}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mtrain_loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(train_loss),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mval_loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(val_loss))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m val_loss \u001b[39m<\u001b[39m best_val_loss:\n",
      "\u001b[1;32m/Users/gyf/JHU/Spring 22/Data Mining/Project/602_proj/GNN.ipynb Cell 7\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (g, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         output \u001b[39m=\u001b[39m model(g, g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y264sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(output, labels)\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/dgl/heterograph.py:2232\u001b[0m, in \u001b[0;36mDGLGraph.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2229\u001b[0m etypes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_etypes(key)\n\u001b[1;32m   2231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(etypes) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2232\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(\u001b[39m'\u001b[39m\u001b[39mInvalid key \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Must be one of the edge types.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(orig_key))\n\u001b[1;32m   2234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(etypes) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2235\u001b[0m     \u001b[39m# no ambiguity: return the unitgraph itself\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     srctype, etype, dsttype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_canonical_etypes[etypes[\u001b[39m0\u001b[39m]]\n",
      "\u001b[0;31mDGLError\u001b[0m: Invalid key \"0\". Must be one of the edge types."
     ]
    }
   ],
   "source": [
    "def smape_loss(output, target):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE) between output and target.\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs(output - target) / (torch.abs(output) + torch.abs(target) + 1e-8))\n",
    "\n",
    "def train(model, train_loader, val_loader, num_epochs, optimizer, criterion, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for i, (g, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(g, g.ndata['x'])\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'train_loss: {:.4f}'.format(train_loss),\n",
    "              'val_loss: {:.4f}'.format(val_loss))\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (g, labels) in enumerate(loader):\n",
    "            output = model(g, g.ndata['x'])\n",
    "            loss += criterion(output, labels).item()\n",
    "    \n",
    "    return loss / len(loader)\n",
    "\n",
    "model = GGN(in_feats=len(features), h_feats=64, num_classes=1).to(device)\n",
    "train_loader = GraphDataLoader(train_graph_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = GraphDataLoader(val_graph_dataset, batch_size=1, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = smape_loss\n",
    "\n",
    "train(model, train_loader, val_loader, num_epochs=50, optimizer=optimizer, criterion=criterion, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 3085 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/gyf/JHU/Spring 22/Data Mining/Project/602_proj/GNN.ipynb Cell 8\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y265sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (g, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y265sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         output \u001b[39m=\u001b[39m model(g, g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y265sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         predictions\u001b[39m.\u001b[39mappend(output\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y265sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# add predictions to the test dataframe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y265sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m df_test[\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m predictions\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 3085 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "# create GraphDataset object for test data\n",
    "test_graph_dataset = GraphDataset(edges=neighb, properties=df_test, features=features, train=True)\n",
    "\n",
    "# create DataLoader for test data\n",
    "test_loader = GraphDataLoader(test_graph_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# loop through test DataLoader and generate predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (g, _) in enumerate(test_loader):\n",
    "        output = model(g, g.ndata['x'])\n",
    "        predictions.append(output.item())\n",
    "\n",
    "# add predictions to the test dataframe\n",
    "df_test['predictions'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
