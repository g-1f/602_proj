{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data import DGLDataset\n",
    "from geopy import distance\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighb = pd.read_csv('gnn_neighbor.csv')\n",
    "neighb['distances'] = neighb.apply(lambda x: ast.literal_eval(x.distances), axis=1)\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_train_NN = df_train[df_train.columns[df_train.columns.str.contains('NN')]]\n",
    "df_train = df_train.loc[:, ~df_train.columns.str.contains('NN')]\n",
    "target_train = pd.read_csv('target_train.csv')\n",
    "df_train = pd.merge(df_train, target_train, on='row_id')\n",
    "\n",
    "df_test = pd.read_csv('df_test.csv')\n",
    "df_test_NN = df_test[df_test.columns[df_test.columns.str.contains('NN')]]\n",
    "df_test = df_test.loc[:, ~df_test.columns.str.contains('NN')]\n",
    "target_test = pd.read_csv('target_test.csv')\n",
    "df_test = pd.merge(df_test, target_test, on='row_id')\n",
    "features = df_train.drop(['row_id', 'cfips', 'microbusiness_density', 'active'], axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.describe()\n",
    "df_train['cfips'] = df_train['cfips'].apply(lambda x: neighb.loc[neighb['original_cfips'] == x, 'cfips'].values[0])\n",
    "df_test['cfips'] = df_test['cfips'].apply(lambda x: neighb.loc[neighb['original_cfips'] == x, 'cfips'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(DGLDataset):\n",
    "    def __init__(self, edges, properties, features, train=True):\n",
    "        self.edges = edges\n",
    "        self.properties = properties\n",
    "        self.features = features\n",
    "        self.train = train\n",
    "        super().__init__(name=\"microbussiness\")\n",
    "\n",
    "    def process(self):\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for index, group in self.properties.groupby('idx'):\n",
    "            filtered_edges = self.edges[self.edges['cfips'].isin(group['cfips'])]\n",
    "\n",
    "            if filtered_edges.empty:\n",
    "                continue\n",
    "\n",
    "            src = []\n",
    "            dst = []\n",
    "            w = []\n",
    "\n",
    "            for _, row in filtered_edges.iterrows():\n",
    "                neighbours = ast.literal_eval(row['neighbors'])\n",
    "                distances = row['distances']\n",
    "\n",
    "                for neighbor, distance in zip(neighbours, distances):\n",
    "                    if neighbor in group['cfips'].values:\n",
    "                        src.append(row['cfips'])\n",
    "                        dst.append(neighbor)\n",
    "                        w.append(distance)\n",
    "\n",
    "            if len(w) == 0:\n",
    "                continue\n",
    "\n",
    "            w = np.array(w)\n",
    "            min_w = np.min(w)\n",
    "            max_w = np.max(w)\n",
    "            w = (w - min_w) / (max_w - min_w)\n",
    "\n",
    "            node_id_mapping = {node_id: i for i, node_id in enumerate(group['cfips'])}\n",
    "            src = [node_id_mapping[x] for x in src]\n",
    "            dst = [node_id_mapping[x] for x in dst]\n",
    "\n",
    "            g = dgl.graph((src, dst), num_nodes=len(group))\n",
    "            g.ndata['x'] = torch.tensor(group[self.features].values, dtype=torch.float) # node feature\n",
    "            g.edata['w'] = torch.tensor(w, dtype=torch.float)  # scalar integer feature\n",
    "            g = dgl.add_self_loop(g)\n",
    "            self.graphs.append(g)\n",
    "            if self.train:\n",
    "                self.labels.append(torch.tensor(group[\"target\"].values, dtype=torch.float))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.train:\n",
    "            return self.graphs[i].to(device), self.labels[i].to(device)\n",
    "\n",
    "        return self.graphs[i].to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.graphs)\n",
    "        else:\n",
    "            return len(self.properties)\n",
    "    \n",
    "train_graph_dataset = GraphDataset(edges=neighb, properties=df_train, features=features)\n",
    "val_graph_dataset = GraphDataset(edges=neighb, properties=df_test, features=features,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GGN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GGN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.convh = nn.ModuleList([GraphConv(h_feats, h_feats) for _ in range(20)])\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        for c in self.convh:\n",
    "            h = c(g, h)\n",
    "            h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.tanh(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss: 0.9576 val_loss: 0.9026\n",
      "Epoch: 0002 train_loss: 0.9660 val_loss: 0.9940\n",
      "Epoch: 0003 train_loss: 0.9696 val_loss: 0.9932\n",
      "Epoch: 0004 train_loss: 0.9595 val_loss: 0.9316\n",
      "Epoch: 0005 train_loss: 0.9733 val_loss: 0.9873\n",
      "Epoch: 0006 train_loss: 0.9664 val_loss: 0.9037\n",
      "Epoch: 0007 train_loss: 0.9654 val_loss: 0.9519\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: 0008 train_loss: 0.9614 val_loss: 0.9117\n",
      "Epoch: 0009 train_loss: 0.9554 val_loss: 0.9573\n",
      "Epoch: 0010 train_loss: 0.9553 val_loss: 0.9489\n",
      "Epoch: 0011 train_loss: 0.9310 val_loss: 0.9348\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "def smape_loss(output, target):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE) between output and target.\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs(output - target) / (torch.abs(output) + torch.abs(target) + 1e-8))\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "def train(model, train_loader, val_loader, num_epochs, optimizer, criterion, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    counter = 0\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for i, (g, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(g, g.ndata['x'])\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'train_loss: {:.4f}'.format(train_loss),\n",
    "              'val_loss: {:.4f}'.format(val_loss))\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (g, labels) in enumerate(loader):\n",
    "            output = model(g, g.ndata['x'])\n",
    "            loss += criterion(output, labels).item()\n",
    "    \n",
    "    return loss / len(loader)\n",
    "\n",
    "model = GGN(in_feats=len(features), h_feats=64, num_classes=1).to(device)\n",
    "train_loader = GraphDataLoader(train_graph_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = GraphDataLoader(val_graph_dataset, batch_size=1, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = smape_loss\n",
    "\n",
    "train(model, train_loader, val_loader, num_epochs=50, optimizer=optimizer, criterion=criterion, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph_dataset = GraphDataset(edges=neighb, properties=df_test, features=features, train=False)\n",
    "test_dataloader = GraphDataLoader(test_graph_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'DGLGraph' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gyf/JHU/Spring 22/Data Mining/Project/602_proj/GNN.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y302sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             predictions\u001b[39m.\u001b[39mextend(out\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y302sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(predictions)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y302sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_predictions \u001b[39m=\u001b[39m predict(model, test_dataloader)\n",
      "\u001b[1;32m/Users/gyf/JHU/Spring 22/Data Mining/Project/602_proj/GNN.ipynb Cell 9\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y302sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y302sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch, graphs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y302sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         batched_graph \u001b[39m=\u001b[39m dgl\u001b[39m.\u001b[39;49mbatch(graphs)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y302sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         out \u001b[39m=\u001b[39m model(batched_graph, batched_graph\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyf/JHU/Spring%2022/Data%20Mining/Project/602_proj/GNN.ipynb#Y302sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         predictions\u001b[39m.\u001b[39mextend(out\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/dgl/batch.py:151\u001b[0m, in \u001b[0;36mbatch\u001b[0;34m(graphs, ndata, edata)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch\u001b[39m(graphs, ndata\u001b[39m=\u001b[39mALL, edata\u001b[39m=\u001b[39mALL):\n\u001b[1;32m     15\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Batch a collection of :class:`DGLGraph` s into one graph for more efficient\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m    graph computation.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m    unbatch\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(graphs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    152\u001b[0m         \u001b[39mraise\u001b[39;00m DGLError(\u001b[39m'\u001b[39m\u001b[39mThe input list of graphs cannot be empty.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    153\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_all(ndata) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(ndata, \u001b[39mlist\u001b[39m) \u001b[39mor\u001b[39;00m ndata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'DGLGraph' has no len()"
     ]
    }
   ],
   "source": [
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, graphs in enumerate(dataloader):\n",
    "            batched_graph = dgl.batch(graphs).to(device)\n",
    "            out = model(batched_graph, batched_graph.ndata['x']).view(-1)\n",
    "            predictions.extend(out.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
